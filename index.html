<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of
Vision-Language Models.">
  <meta name="keywords" content="MCP, Multi-Cache, Prototype Learning, Test-Time Adaption, Vision-Language Models, ICCV 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of
Vision-Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zhaihaotian.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://zhaihaotian.github.io/CRG-ICME25/">
            CRG
          </a>
          <!-- <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of
Vision-Language Models</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">Xinyu Chen<sup>1,2</sup>*,</span>
            <span class="author-block">Haotian Zhai<sup>2,3</sup>*,</span>
            <span class="author-block">Can Zhang<sup>2</sup>,</span>
            <span class="author-block">Xiupeng Shi<sup>1,†</sup>,</span>
            <span class="author-block">Ruirui Li<sup>2,†</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai University,</span>
            <span class="author-block"><sup>2</sup>Beijing University of Chemical Technology,</span>
            <span class="author-block"><sup>3</sup>University of Minnesota</span><br>
            <span class="author-block"><sup>*</sup>Equal contribution,</span>
            <span class="author-block"><sup>†</sup>Corresponding author</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block" style="vertical-align: middle;">
                <a href="https://arxiv.org/pdf/2508.01225"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block" style="vertical-align: middle;">
                <a href="https://arxiv.org/abs/2508.01225"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
               <span class="link-block" style="vertical-align: middle;">
                <span class="external-link button is-normal is-rounded is-dark is-disabled">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Coming Soon</span>
                </span>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section is-light">
    <!-- 方法流程圖 -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-centered">
          <!-- 
            *** 重要步驟 ***
            請將下面的 'your_method_image.png' 換成您方法圖的檔名。
            例如: <img src="./static/images/mcp_plus_plus.png" ... >
            請確認圖片已經放在 static/images 資料夾中。
          -->
          <img src="./static/images/mcp.png" alt="Our Method Diagram" style="width: 90%; max-width: 800px;"/>
        </div>
      </div>
    </div>

    <!-- 流程圖下方的文字說明 -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <div style="width: 90%; max-width: 800px; margin: 0 auto;"></div>
          <p>
            An overview of our MCP++ method. The Entropy Cache stores low-entropy samples, the Align Cache retains samples closest to the prototype center, and the Negative Cache preserves high-entropy samples after pseudo-label refinement through a reflecting mechanism. We introduce textual and visual prototypes refined with learnable residuals to construct the prototype center, which is optimized using alignment loss, contrastive loss and entropy loss. The final prediction is derived through a retrieval mechanism that aggregates similarity scores from negative cache features, prototype center and adaptive cache features.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- 
      解決方案：我們將圖片和文字分成兩行來處理。
      第一行只放圖片，確保它們頂部對齊。
      第二行只放文字，確保它們也彼此對齊。
    -->

    <!-- 第一行：只放圖片 -->
    <div class="columns is-centered is-vcentered">
      <!-- Column 1: 第一張圖片 -->
      <div class="column">
        <div class="content">
          <!-- 您的第一張圖片 -->
          <img src="./static/images/acc.png" alt="Figure 1"/>
        </div>
      </div>

      <!-- Column 2: 第二張圖片 -->
      <div class="column">
        <div class="content">
          <!-- 您的第二張圖片 -->
          <img src="./static/images/intro.png" alt="Figure 2"/>
        </div>
      </div>
    </div>

    <!-- 第二行：只放文字說明 -->
    <div class="columns is-centered">
      <!-- Column 1: 第一段文字 -->
      <div class="column">
        <p class="has-text-justified">
          <strong>Figure 1:</strong> Illustration of the average classification accuracy, test-time training GFLOPs, and FPS for different methods on cross-dataset classification tasks. For prompt-based and cache-based methods, the icon sizes denote the FPS values.
        </p>
      </div>
      <!-- Column 2: 第二段文字 -->
      <div class="column">
        <p class="has-text-justified">
          <strong>Figure 2:</strong> The x-axis represents the compactness of test data(defined as the inverse of the average distance between each sample and its class center), and the y-axis represents the accuracy improvement of TDA relative to zero-shot CLIP. A positive correlation is observed on the curve and two t-SNE visualizations on EuroSAT and Aircraft datasets.
        </p>
      </div>
    </div>

  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           In zero-shot setting, test-time adaptation adjusts pre-trained models using unlabeled data from the test phase to enhance performance on unknown test distributions. Existing cache-enhanced TTA methods rely on a low-entropy criterion to select samples for prototype construction, assuming intra-class compactness. However, low-entropy samples may be unreliable under distribution shifts, and the resulting prototypes may not ensure compact intra-class distributions. This study identifies a positive correlation between cache-enhanced performance and intra-class compactness. Based on this observation, we propose a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) featuring three caches: an entropy cache for initializing prototype representations with low-entropy samples, an align cache for integrating visual and textual information to achieve compact intra-class distributions, and a negative cache for prediction calibration using high-entropy samples. We further developed MCP++, a framework incorporating cross-modal prototype alignment and residual learning, introducing prototype residual fine-tuning. Comparative and ablation experiments across 15 downstream tasks demonstrate that the proposed method and framework achieve state-of-the-art generalization performance.
          </p>
        </div>
      </div>
    </div>
    
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Contribution 標題 -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contributions</h2>
      </div>
    </div>
    <!-- Contribution 內容 -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <!-- 您可以在這裡用列表來闡述您的主要貢獻 -->
          <ul>
            <li>
               We identify a positive correlation between the performance of the caching mechanism and the compactness of cached samples, offering insights to improve adaptive generalization during testing.
            </li>
            <li>
               We propose a multi-cache enhanced prototype-based TTA method, leveraging the entropy, align, and negative caches to achieve more compact intra-class distributions and improve sample prediction accuracy. 
            </li>
            <li>
              We introduce residual fine‑tuning of visual and textual prototypes within our multi‑cache enhancement framework, resulting in more effective alignment between visual and language modalities.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section is-light">
  <div class="container is-max-desktop">
    <!-- Results 標題 -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
      </div>
    </div>

    <!-- 表格 1 圖片 -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-centered">
          <img src="./static/images/cd.png" alt="Table 1 Results" style="width: 90%; max-width: 800px;"/>
        </div>
      </div>
    </div>

    <!-- 表格 1 說明文字 -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <div style="width: 90%; max-width: 800px; margin: 0 auto;">
            <p>
              <strong>Table 1:</strong> Results on the Cross-Domain Benchmark. Top-1 accuracy (%) results are presented for all evaluated methods employing the ViT-B/16 visual backbone of CLIP. The best results are highlighted in bold.
            </p>
          </div>
        </div>
      </div>
    </div>

    <!-- 表格 2 圖片 (與上方表格間隔) -->
    <div class="columns is-centered" style="margin-top: 2rem;">
      <div class="column is-full-width">
        <div class="content has-text-centered">
          <img src="./static/images/im.png" alt="Table 2 Results" style="width: 90%; max-width: 800px;"/>
        </div>
      </div>
    </div>

    <!-- 表格 2 說明文字 -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <div style="width: 90%; max-width: 800px; margin: 0 auto;">
            <p>
              <strong>Table 2:</strong> Results on the OOD Benchmark. Top-1 accuracy (%) results are presented for all evaluated methods employing the ViT-B/16 visual backbone of CLIP. The best results are highlighted in bold.
            </p>
          </div>
        </div>
      </div>
    </div>

  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2025multi,
    title={Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models},
    author={Chen, Xinyu and Zhai, Haotian and Zhang, Can and Shi, Xiupeng and Li, Ruirui},
    journal={arXiv preprint arXiv:2508.01225},
    year={2025}
  }

</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This page was adopted from <a
              href="https://github.com/nerfies/nerfies.github.io"> Nerfies</a> project page,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
